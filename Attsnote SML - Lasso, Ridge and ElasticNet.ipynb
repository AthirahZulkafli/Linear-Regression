{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5a7a32-4b0d-4c3a-9cfb-85c0c189559f",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning by @attzulkafli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6788c-1ad9-49f5-92cc-f796af5516ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lasso, Ridge, and Elastic Net Regression: Regularization\n",
    "\n",
    "\n",
    "**Lasso Regression:**\n",
    "\n",
    "**LASSO** stands for ``Least Absolute Shrinkage and Selection Operator`` where emphasis on the 2 key words – ‘absolute‘ and ‘selection‘.\n",
    "\n",
    "Lasso regression performs **L1 regularization**, i.e. it adds a factor of sum of absolute value of coefficients in the optimization objective. \n",
    "\n",
    "- Minimize Error: $$\\sum_{i=1}^{m} {(y - \\hat{y})^2} + \\alpha\\sum_{i=1}^{n} {|\\beta_i|} = \\sum_{i=1}^{m} {(y -  \\beta_0 - \\beta_1  x_1 - \\beta_2  x_2 - ... - \\beta_n  x_n )^2} + \\alpha\\sum_{i=1}^{n} {|\\beta_i|}$$\n",
    "\n",
    "Thus, lasso regression optimizes the following:\n",
    "\n",
    "**Objective = RSS + α * (sum of absolute value of coefficients)**\n",
    "\n",
    "Here, α (alpha) works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients. Like that of ridge, α can take various values. Lets iterate it here briefly:\n",
    "\n",
    "1. α = 0: Same coefficients as simple linear regression\n",
    "2. α = ∞: All coefficients zero (same logic as before)\n",
    "3. 0 < α < ∞: coefficients between 0 and that of simple linear regression\n",
    "\n",
    "**Ridge Regression:**\n",
    "\n",
    "\n",
    "As mentioned before, ``ridge regression`` performs ‘L2 regularization‘, i.e. it adds a factor of sum of squares of coefficients in the optimization objective.\n",
    "\n",
    "- Minimize Error: $$\\sum_{i=1}^{m} {(y - \\hat{y})^2}+ \\alpha \\sum_{i=1}^{n} {\\beta_i^2} = \\sum_{i=1}^{m} {(y -  \\beta_0 - \\beta_1  x_1 - \\beta_2  x_2 - ... - \\beta_n  x_n )^2} + \\alpha \\sum_{i=1}^{n} {\\beta_i^2}$$\n",
    "\n",
    "Thus, ridge regression optimizes the following:\n",
    "\n",
    "**Objective = RSS + α * (sum of square of coefficients)**\n",
    "\n",
    "Here, α (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients. α can take various values:\n",
    "\n",
    "1. α = 0:\n",
    "    - The objective becomes same as simple linear regression.\n",
    "    - We’ll get the same coefficients as simple linear regression.\n",
    "2. α = ∞:\n",
    "    - The coefficients will be zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.\n",
    "3. 0 < α < ∞:\n",
    "    - The magnitude of α will decide the weightage given to different parts of objective.\n",
    "    - The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
    "\n",
    "**Elastic Net Regression:**\n",
    "\n",
    "- Minimize Error: $$\\sum_{i=1}^{m} {(y - \\hat{y})^2}+ \\lambda_1 \\sum_{i=1}^{n} {|\\beta_i|} + \\lambda_2 \\sum_{i=1}^{n} {\\beta_i^2} = \\sum_{i=1}^{m} {(y -  \\beta_0 - \\beta_1  x_1 - \\beta_2  x_2 - ... - \\beta_n  x_n )^2} + \\lambda_1 \\sum_{i=1}^{n} {|\\beta_i|} + \\lambda_2 \\sum_{i=1}^{n} {\\beta_i^2}$$\n",
    "\n",
    "In `sklearn`, the relationship between $\\lambda_1$ and $\\lambda_2$ is defined by two parameters in `ElasticNet` function `alpha` and `l1_ratio` where:\n",
    "$$\\alpha = \\lambda_1 + \\lambda_2$$\n",
    "and $$ l1-ratio = \\frac {\\lambda_1}{(\\lambda_1 + \\lambda_2)}$$\n",
    "\n",
    "For example, if $\\alpha = 1$, and l1_ratio = 0.3, then:\n",
    "$$ {\\lambda_1 + \\lambda_2 = 1} \\\\ { \\frac {\\lambda_1}{(\\lambda_1 + \\lambda_2)} = 0.3}$$\n",
    "Therefore: \n",
    "$$ {\\lambda_1 = 0.3} \\\\  {\\lambda_2 = 0.7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbf7bac3-652b-4cad-b01e-355ddfae0449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2113d91d-fa5b-4915-b736-2ffced18bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialLasso(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         Lasso(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b0cddd5-d8d5-4bdc-88f1-d010dc546dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRidge(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         Ridge(**kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56d12969-a475-4599-9513-0fe1099dbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialElastic(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         ElasticNet(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23f711-7910-4611-babd-cc528abc4ade",
   "metadata": {},
   "source": [
    " **Example - Lasso on Boston with Polynomials**\n",
    "\n",
    "- Load again the boston dataset from sklearn.\n",
    "- Divide the data into 30% test and 70% train set.\n",
    "- Fit a Polynomial Lasso Regression on the train data\n",
    "    - Use degree=2, alpha=0.1, max_iter=100000\n",
    "- What is the R2 for train and test? How many features were selected?\n",
    "- Now try:\n",
    "    - change PolynomialLasso to set interaction_only=True in PolynomialFeatures\n",
    "    - degree=3, alpha=1, max_iter=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f68d6bf-2d8c-45fd-982b-68d1acd7a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fef4950-6c1e-4db2-beb1-6321cebea5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2, y1, y2 = train_test_split(X, y,random_state=0,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bd774db-2c54-481a-a081-c511a9e5580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.947634400969777\n",
      "Test score 0.6444508360125614\n",
      "Features all 105\n",
      "Features used 91\n",
      "Features NOT used 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 839.8364481115291, tolerance: 3.000663776836158\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "Lasso_Poly2_boston = PolynomialLasso(2, alpha = 0.005, max_iter=1e5)\n",
    "Lasso_Poly2_boston.fit(X1, y1)\n",
    "print(\"Train score\", Lasso_Poly2_boston.score(X1, y1))\n",
    "print(\"Test score\", Lasso_Poly2_boston.score(X2,y2))\n",
    "k = Lasso_Poly2_boston.steps[1][1].coef_\n",
    "print(\"Features all\", len(k))\n",
    "print(\"Features used\", sum(k!= 0))\n",
    "print(\"Features NOT used\", sum(k == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6e0a1eb-4c33-4089-aba0-564b79add583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.983151786505286\n",
      "Test score 0.1371925206481145\n",
      "Features all 560\n",
      "Features used 238\n",
      "Features NOT used 322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 358.06123523646136, tolerance: 3.000663776836158\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "Lasso_Poly3_boston = PolynomialLasso(3, alpha = 1, max_iter=1e5)\n",
    "Lasso_Poly3_boston.fit(X1, y1)\n",
    "\n",
    "print(\"Train score\", Lasso_Poly3_boston.score(X1, y1))\n",
    "print(\"Test score\", Lasso_Poly3_boston.score(X2,y2))\n",
    "k = Lasso_Poly3_boston.steps[1][1].coef_\n",
    "print(\"Features all\", len(k))\n",
    "print(\"Features used\", sum(k != 0))\n",
    "print(\"Features NOT used\", sum(k == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2305df-519d-4ccf-a7a3-509ab5e219b0",
   "metadata": {},
   "source": [
    "**Exercise: Ridge & ElasticNet on Boston with Polynomials**\n",
    "- Fit a Polynomial Ridge Regression on the train data\n",
    "    - Use degree=2, alpha=0.1, max_iter=100000\n",
    "- What is the R2 for train and test? How many features were selected?\n",
    "- Now try:\n",
    "    - change PolynomialLasso to set interaction_only=True in PolynomialFeatures\n",
    "    - degree=3, alpha=1, max_iter=100000\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3ac87ec-f5b6-4432-9f9b-3c6fc7c611bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9506157246830967\n",
      "Test score 0.614488986368475\n",
      "Features all 105\n",
      "Features used 104\n",
      "Features NOT used 1\n"
     ]
    }
   ],
   "source": [
    "Ridge_Poly2_boston = PolynomialRidge(2, alpha = 0.1, max_iter=1e5)\n",
    "Ridge_Poly2_boston.fit(X1, y1)\n",
    "print(\"Train score\", Ridge_Poly2_boston.score(X1, y1))\n",
    "print(\"Test score\", Ridge_Poly2_boston.score(X2,y2))\n",
    "k = Ridge_Poly2_boston.steps[1][1].coef_\n",
    "print(\"Features all\", len(k))\n",
    "print(\"Features used\", sum(k != 0))\n",
    "print(\"Features NOT used\", sum(k == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72fb4c7c-32e7-4bd7-bb34-bbf52f329598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9455122611062422\n",
      "Test score -3.855119853801522\n",
      "Features all 560\n",
      "Features used 559\n",
      "Features NOT used 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    }
   ],
   "source": [
    "Ridge_Poly3_boston = PolynomialRidge(3, alpha = 1, max_iter=1e5)\n",
    "Ridge_Poly3_boston.fit(X1, y1)\n",
    "print(\"Train score\", Ridge_Poly3_boston.score(X1, y1))\n",
    "print(\"Test score\", Ridge_Poly3_boston.score(X2,y2))\n",
    "k = Ridge_Poly3_boston.steps[1][1].coef_\n",
    "print(\"Features all\", len(k))\n",
    "print(\"Features used\", sum(k != 0))\n",
    "print(\"Features NOT used\", sum(k == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cddf92d-a7da-4a58-a7ed-72b1f36dcf8e",
   "metadata": {},
   "source": [
    "- Fit a Polynomial Elastic Net Regression on the train data\n",
    "    - Use degree=2, alpha=0.1, max_iter=100000\n",
    "- What is the R2 for train and test? How many features were selected?\n",
    "- Now try:\n",
    "    - change PolynomialLasso to set interaction_only=True in PolynomialFeatures\n",
    "    - degree=3, alpha = 1, l1_ratio=0.5, max_iter=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f37f86d-7bc1-4688-8490-bffd6401fa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9163405765640357\n",
      "Test score 0.7664562575950314\n",
      "Features all 105\n",
      "Features used 54\n",
      "Features NOT used 51\n"
     ]
    }
   ],
   "source": [
    "Elastic_Poly2_boston = PolynomialElastic(2, alpha = 1, l1_ratio=0.5, max_iter=1e5)\n",
    "Elastic_Poly2_boston.fit(X1, y1)\n",
    "print(\"Train score\", Elastic_Poly2_boston.score(X1, y1))\n",
    "print(\"Test score\", Elastic_Poly2_boston.score(X2,y2))\n",
    "k = Elastic_Poly2_boston.steps[1][1].coef_\n",
    "print(\"Features all\", len(k))\n",
    "print(\"Features used\", sum(Elastic_Poly2_boston.steps[1][1].coef_ != 0))\n",
    "print(\"Features NOT used\", sum(Elastic_Poly2_boston.steps[1][1].coef_ == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2806ac4-d8b5-4f86-8d77-05ea48163fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9858405664684327\n",
      "Test score -0.04198637963927765\n",
      "Features all 560\n",
      "Features used 266\n",
      "Features NOT used 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299.47100671870277, tolerance: 3.000663776836158\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "Elastic_Poly3_boston = PolynomialElastic(3, alpha = 1, l1_ratio=0.5, max_iter=1e5)\n",
    "Elastic_Poly3_boston.fit(X1, y1)\n",
    "print(\"Train score\", Elastic_Poly3_boston.score(X1, y1))\n",
    "print(\"Test score\", Elastic_Poly3_boston.score(X2,y2))\n",
    "k = Elastic_Poly3_boston.steps[1][1].coef_\n",
    "print(\"Features all\", len(k))\n",
    "print(\"Features used\", sum(Elastic_Poly3_boston.steps[1][1].coef_ != 0))\n",
    "print(\"Features NOT used\", sum(Elastic_Poly3_boston.steps[1][1].coef_ == 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
